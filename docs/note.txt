

==============================S1==============================

-fep_gateway의 워커I/O 1스레드로 동작할 경우 클라이언트 연결이 10개 인경우 
-즉 1개의 IO 스레드가 패킷 처리후 epoll_wait 등 진입 후 스위칭 되는 경우를 관찰


❯ pidstat -w -t -p $(pgrep -x fep_gateway) 1
Linux 6.18.7-200.fc43.x86_64 (fedora) 	2026년 02월 09일 	_x86_64_	(16 CPU)

13시 09분 24초   UID      TGID       TID   cswch/s nvcswch/s  Command
13시 09분 25초  1000     34197         -      0.00      0.00  fep_gateway
13시 09분 25초  1000         -     34197      0.00      0.00  |__fep_gateway
13시 09분 25초  1000         -     34199      0.00      0.00  |__fep_gateway
13시 09분 25초  1000         -     34201      0.00      0.00  |__fep_gateway
13시 09분 25초  1000         -     34202  43335.00      8.00  |__fep_gateway
13시 09분 26초  1000     34197         -      0.00      0.00  fep_gateway
13시 09분 26초  1000         -     34197      0.00      0.00  |__fep_gateway
13시 09분 26초  1000         -     34199      0.00      0.00  |__fep_gateway
13시 09분 26초  1000         -     34201      0.00      0.00  |__fep_gateway
13시 09분 26초  1000         -     34202  44110.00      8.00  |__fep_gateway
13시 09분 27초  1000     34197         -      0.00      0.00  fep_gateway
13시 09분 27초  1000         -     34197      0.00      0.00  |__fep_gateway
13시 09분 27초  1000         -     34199      0.00      0.00  |__fep_gateway
13시 09분 27초  1000         -     34201      0.00      0.00  |__fep_gateway
13시 09분 27초  1000         -     34202  43743.00     10.00  |__fep_gateway
13시 09분 28초  1000     34197         -      0.00      0.00  fep_gateway
....
==========================================================================



==============================S2==============================

-fep_gateway의 워커I/O 2스레드로 동작할 경우 클라이언트 연결이 10개 인경우 

❯ pidstat -w -t -p $(pgrep -x fep_gateway) 1
Linux 6.18.7-200.fc43.x86_64 (fedora) 	2026년 02월 09일 	_x86_64_	(16 CPU)

13시 13분 58초   UID      TGID       TID   cswch/s nvcswch/s  Command
13시 13분 59초  1000     37325         -      0.00      0.00  fep_gateway
13시 13분 59초  1000         -     37325      0.00      0.00  |__fep_gateway
13시 13분 59초  1000         -     37328      0.00      0.00  |__fep_gateway
13시 13분 59초  1000         -     37330      0.00      0.00  |__fep_gateway
13시 13분 59초  1000         -     37331  21861.00      4.00  |__fep_gateway
13시 13분 59초  1000         -     37332  21861.00      5.00  |__fep_gateway

....
==========================================================================



==============================S3==============================

-fep_gateway의 워커I/O 2스레드로 동작할 경우 클라이언트 연결이 10개 인경우 
- 이번 경우는 의도적으로 클라이언트에서 받은 패킷을 다른 워커의 Taskqueue에 넣고 거래소로 보내도록 조정
- 결과를 보면 epoll_wait에 대기중인 다른 워커의 큐에 작업을 전달 후 깨우는 비용에 대한 스위칭 비용이 증가했음을 알 수 있음

❯ pidstat -w -t -p $(pgrep -x fep_gateway) 1
Linux 6.18.7-200.fc43.x86_64 (fedora) 	2026년 02월 09일 	_x86_64_	(16 CPU)

13시 12분 36초   UID      TGID       TID   cswch/s nvcswch/s  Command
13시 12분 37초  1000     36867         -      0.00      0.00  fep_gateway
13시 12분 37초  1000         -     36867      0.00      0.00  |__fep_gateway
13시 12분 37초  1000         -     36870      0.00      0.00  |__fep_gateway
13시 12분 37초  1000         -     36872      0.00      0.00  |__fep_gateway
13시 12분 37초  1000         -     36873  31834.00      0.00  |__fep_gateway
13시 12분 37초  1000         -     36874  27858.00      0.00  |__fep_gateway
....
==========================================================================

의견: TaskQueue에 작업 전달 과정의 최적화가 필요할 듯 하다..








=====================================================================================================================

TPS TEST
…/server_engine on  dev [✘!?] via △ v3.31.10 
❯ python ./scripts/monitor_system.py
...
---------------------------------------------------------------------------
17:08:08   | GATEWAY    | 3,515,073    | 248,619    | 3,515,059    | 248,619   
17:08:08   | EXCHANGE   | 1,757,622    | 124,315    | 1,757,619    | 124,314   
---------------------------------------------------------------------------
17:08:09   | GATEWAY    | 3,763,255    | 248,182    | 3,763,242    | 248,183   
17:08:09   | EXCHANGE   | 1,881,719    | 124,097    | 1,881,717    | 124,098   
---------------------------------------------------------------------------
17:08:10   | GATEWAY    | 4,015,024    | 251,769    | 4,015,011    | 251,769   
17:08:10   | EXCHANGE   | 2,007,603    | 125,884    | 2,007,601    | 125,884   
---------------------------------------------------------------------------
17:08:11   | GATEWAY    | 4,265,588    | 250,564    | 4,265,575    | 250,564   
17:08:11   | EXCHANGE   | 2,132,876    | 125,273    | 2,132,874    | 125,273   

^C
Stopped.

…/server_engine on  dev [✘!?] via △ v3.31.10 took 20s 
❯ python ./scripts/monitor_system.py
Monitoring System Started... (Targets: 2)
...
17:12:24   | GATEWAY    | 1,842,559    | 137,912    | 1,842,547    | 137,912   
17:12:24   | EXCHANGE   | 921,332      | 68,965     | 921,330      | 68,964    
---------------------------------------------------------------------------
17:12:25   | GATEWAY    | 1,980,834    | 138,275    | 1,980,822    | 138,275   
17:12:25   | EXCHANGE   | 990,459      | 69,127     | 990,458      | 69,128    
---------------------------------------------------------------------------
17:12:26   | GATEWAY    | 2,118,032    | 137,198    | 2,118,021    | 137,199   
17:12:26   | EXCHANGE   | 1,059,116    | 68,657     | 1,059,115    | 68,657    
---------------------------------------------------------------------------
17:12:27   | GATEWAY    | 2,253,696    | 135,664    | 2,253,685    | 135,664   
17:12:27   | EXCHANGE   | 1,126,892    | 67,776     | 1,126,890    | 67,775    
---------------------------------------------------------------------------
17:12:28   | GATEWAY    | 2,389,612    | 135,916    | 2,389,600    | 135,915   
17:12:28   | EXCHANGE   | 1,194,860    | 67,968     | 1,194,859    | 67,969    
=====================================================================================================================
GATEWAY,EXCHANGE 각각 워커스레드를 각각 1개와 2개로 나누고 클라이언트에서는 패킷을 계속 전송하는 방식으로 진행
처리량이 거의 2배가까이 증가했다. 참고로 TaskQueue 작업 전달은 하지 않고 IO워커가 소유한 세션을 그대로 처리하도록 했음
